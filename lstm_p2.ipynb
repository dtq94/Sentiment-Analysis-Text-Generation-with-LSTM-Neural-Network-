{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4657115b",
   "metadata": {},
   "source": [
    "# **Many-to-One LSTM for Sentiment Analysis and Text Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2378f",
   "metadata": {},
   "source": [
    "## **Project Overview**\n",
    "\n",
    "In this project, we will explore the fascinating world of sentiment analysis and text generation using many-to-one Long Short-Term Memory (LSTM) neural networks.\n",
    "\n",
    "Sentiment Detection:\n",
    "We will dive into sentiment analysis by training an LSTM model to analyze airline sentiments. The dataset consists of sentiment labels (0 or 1) and corresponding text reviews. We will preprocess the data, converting it into a numerical representation using the bag-of-words technique. Through the many-to-one LSTM architecture, we will predict sentiment labels based on the textual input.\n",
    "\n",
    "Text Generation:\n",
    "Next, we will venture into the realm of text generation. Using the beloved classic \"Alice's Adventures in Wonderland\" as our training text, we will prepare and structure the data to facilitate the training of many-to-one LSTMs. These LSTMs will learn the patterns and sequences in the text, enabling us to generate new text based on a given prompt. We will focus specifically on next-word prediction, allowing us to generate coherent and contextually relevant sentences.\n",
    "\n",
    "During our exploration, we will address the challenges of text generation, including the dynamic nature of language and the presence of multiple words with similar meanings. To overcome these challenges, we will utilize techniques such as entropy scaling and softmax temperature. By adjusting the temperature parameter, we can control the randomness and diversity of the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766e1f9",
   "metadata": {},
   "source": [
    "## **Approach**\n",
    "\n",
    "* Sentiment Analysis:\n",
    "\n",
    "    * Dataset:\n",
    "        * Obtain the airline sentiment dataset consisting of sentiment labels (0 or 1) and corresponding text reviews.\n",
    "\n",
    "    * Preprocessing:\n",
    "        * Perform data preprocessing tasks, including text cleaning, tokenization, and removing stop words.\n",
    "        * Convert the text reviews into a bag-of-words representation.\n",
    "\n",
    "    * Many-to-One LSTM:\n",
    "        * Utilize many-to-one LSTM architecture to train the sentiment detection model.\n",
    "        * Feed the bag-of-words representation of the text reviews as input to the LSTM.\n",
    "\n",
    "    * Training:\n",
    "        * Split the dataset into training and testing sets.\n",
    "        * Train the LSTM model using the training set.\n",
    "        * Evaluate the model's performance on the testing set.\n",
    "\n",
    "* Text Generation:\n",
    "\n",
    "    * Dataset:\n",
    "        * Obtain the \"Alice's Adventures in Wonderland\" text dataset.\n",
    "\n",
    "    * Preparing and Structuring:\n",
    "        * Preprocess the text data by cleaning, tokenizing, and structuring the sentences and phrases.\n",
    "        * Create sequences \n",
    "\n",
    "    * Many-to-One LSTM:\n",
    "        * Implement many-to-one LSTM architecture for text generation.\n",
    "        * Train the LSTM model using the prepared dataset.\n",
    "\n",
    "    * The Problem with Text Generation:\n",
    "        * Understand the challenges associated with text generation, such as the variability of language and the dependence on context, style, and word choice.\n",
    "        * Recognize that natural language utilizes a wide variety of words, which may have similar meanings and require careful consideration during generation.\n",
    "\n",
    "    * Randomness through Entropy Scaling:\n",
    "        * Explore the concept of entropy scaling to introduce controlled randomness into text generation.\n",
    "\n",
    "    * Softmax Temperature:\n",
    "        * Introduce the concept of softmax temperature, a hyperparameter used to control the randomness of predictions in LSTMs and neural networks.\n",
    "        * Predicting using the Temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1896a96",
   "metadata": {},
   "source": [
    "## **Important Libraries**\n",
    "\n",
    "* **TensorFlow**: TensorFlow is a popular open-source library for machine learning and deep learning. It provides a flexible framework for building and training various types of neural networks. Refer to the [TensorFlow documentation](https://www.tensorflow.org/) for more information.\n",
    "\n",
    "* **NumPy**: NumPy is a fundamental library for scientific computing in Python. It provides support for large, multi-dimensional arrays and a collection of mathematical functions to manipulate and analyze the data efficiently. Refer to the [NumPy documentation](https://numpy.org/) for more information.\n",
    "\n",
    "* **Scikit-learn**: Scikit-learn is a comprehensive machine learning library in Python. It offers various algorithms for classification, regression, clustering, and dimensionality reduction, along with utilities for model evaluation and preprocessing. Refer to the [scikit-learn documentation](https://scikit-learn.org/stable/) for more information.\n",
    "\n",
    "* **Seaborn**: Seaborn is a data visualization library based on Matplotlib. It provides a high-level interface for creating attractive and informative statistical graphics. Refer to the [Seaborn documentation](https://seaborn.pydata.org/) for more information.\n",
    "\n",
    "* **Matplotlib**: Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python. Refer to the [Matplotlib documentation](https://matplotlib.org/) for more information.\n",
    "\n",
    "* **Pandas**: Pandas is a powerful library for data manipulation and analysis in Python. Refer to the [Pandas documentation](https://pandas.pydata.org/) for more information.\n",
    "\n",
    "* **NLTK**: NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. Refer to the [NLTK documentation](https://www.nltk.org/) for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbdc20e",
   "metadata": {},
   "source": [
    "## **Install Packages**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7906baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2ba5fa-0e47-4d2d-bb8a-3e5ae32b3935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88470f0",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84fc61e6-2e58-4038-b7d4-904a796a1062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical, pad_sequences\n",
    "from keras.layers import Embedding\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from projectpro import save_point, checkpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d9b4a8-9643-4fc1-a87d-4814b446d231",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/DanyTQ/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "tf.keras.backend.set_image_data_format(\"channels_last\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909a15f",
   "metadata": {},
   "source": [
    "## **Refresher: Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb86619",
   "metadata": {},
   "source": [
    "### **Neural Network Architecture**\n",
    "\n",
    "Neural networks, also known as artificial neural networks or simply neural nets, are computational models inspired by the structure and functionality of the human brain. They are widely used in machine learning and deep learning for solving complex problems across various domains.\n",
    "\n",
    "#### **Basic Structure**\n",
    "\n",
    "A neural network consists of interconnected layers of artificial neurons, also known as nodes or units. The layers are organized into an input layer, one or more hidden layers, and an output layer. The input layer receives the input data, the hidden layers process the information, and the output layer produces the final predictions or outputs.\n",
    "\n",
    "#### **Neurons and Connections**\n",
    "\n",
    "Each neuron in a neural network performs a computation on its inputs and produces an output. The neurons in one layer are connected to the neurons in the subsequent layer through weighted connections. These weights determine the strength of the connections and are adjusted during the training process to optimize the network's performance.\n",
    "\n",
    "\n",
    "#### **Mathematics of a Basic Neural Network**\n",
    "\n",
    "A basic neural network consists of multiple layers of neurons connected by weighted connections. Let's consider a neural network with one input layer, one hidden layer, and one output layer.\n",
    "\n",
    "#### **Notation**:\n",
    "\n",
    "- Input layer: $X = [x_1, x_2, ..., x_n]$, where $x_i$ represents the $i$-th input feature.\n",
    "- Hidden layer: $H = [h_1, h_2, ..., h_m]$, where $h_i$ represents the $i$-th neuron in the hidden layer.\n",
    "- Output layer: $Y = [y_1, y_2, ..., y_k]$, where $y_i$ represents the $i$-th output neuron.\n",
    "\n",
    "#### **Forward Propagation**:\n",
    "\n",
    "The weighted sum of inputs for a neuron in the hidden layer is calculated as:\n",
    "\n",
    "$$z_j = \\sum_{i=1}^{n} w_{ji}^{(1)} x_i + b_j^{(1)}$$\n",
    "\n",
    "where $w_{ji}^{(1)}$ represents the weight connecting the $i$-th input to the $j$-th neuron in the hidden layer, and $b_j^{(1)}$ is the bias term for the $j$-th neuron.\n",
    "\n",
    "The output of each neuron in the hidden layer is obtained by applying an activation function $\\sigma$:\n",
    "\n",
    "$$h_j = \\sigma(z_j)$$\n",
    "\n",
    "Similarly, the weighted sum of inputs for a neuron in the output layer is calculated as:\n",
    "\n",
    "$$z_k = \\sum_{j=1}^{m} w_{kj}^{(2)} h_j + b_k^{(2)}$$\n",
    "\n",
    "where $w_{kj}^{(2)}$ represents the weight connecting the $j$-th neuron in the hidden layer to the $k$-th neuron in the output layer, and $b_k^{(2)}$ is the bias term for the $k$-th neuron.\n",
    "\n",
    "The output of each neuron in the output layer is obtained by applying an activation function $\\sigma$:\n",
    "\n",
    "$$y_k = \\sigma(z_k)$$\n",
    "\n",
    "#### **Activation Functions:**\n",
    "\n",
    "Activation functions are a vital component of neural networks. They introduce non-linearity and enable the neural network to learn complex relationships in the data. Here are a few reasons why activation functions are necessary:\n",
    "\n",
    "1. **Non-Linearity**: Without activation functions, the neural network would only be able to approximate linear functions, limiting its learning capacity. Activation functions allow the network to model non-linear relationships between inputs and outputs.\n",
    "\n",
    "2. **Normalization**: Activation functions can normalize the output of a neuron, ensuring that the values fall within a desired range. This can help in stabilizing the learning process and improving the convergence of the network.\n",
    "\n",
    "3. **Differentiability**: Activation functions are differentiable, which is essential for training neural networks using gradient-based optimization algorithms like backpropagation. The gradients of the activation functions help in updating the weights and biases during the training process.\n",
    "\n",
    "\n",
    "\n",
    "Common activation functions used in neural networks include the sigmoid function, tanh (hyperbolic tangent) function, and Rectified Linear Unit (ReLU) function.\n",
    "\n",
    "By applying activation functions after the weighted sum of inputs, neural networks can model complex relationships and make non-linear predictions, enabling them to solve a wide range of problems.\n",
    "\n",
    "\n",
    "#### **Feedforward Propagation**\n",
    "\n",
    "In feedforward propagation, the information flows through the network in the forward direction, starting from the input layer and passing through the hidden layers until it reaches the output layer. Each neuron receives inputs from the previous layer, computes its weighted sum, applies the activation function, and passes the output to the next layer.\n",
    "\n",
    "#### **Backpropagation**\n",
    "\n",
    "Backpropagation is an algorithm used to train neural networks by adjusting the weights based on the calculated gradients of the loss function with respect to the weights. It involves computing the error between the predicted outputs and the actual targets and propagating this error backward through the network to update the weights.\n",
    "\n",
    "#### **Loss Functions**\n",
    "\n",
    "Loss functions quantify the difference between the predicted outputs of the neural network and the actual targets. Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy for classification tasks. The choice of the loss function depends on the nature of the problem being solved.\n",
    "\n",
    "#### **Optimization Algorithms**\n",
    "\n",
    "Optimization algorithms, such as stochastic gradient descent (SGD) and its variants (e.g., Adam, RMSprop), are used to minimize the loss function and update the weights of the neural network during training. These algorithms adjust the weights iteratively based on the gradients computed through backpropagation.\n",
    "\n",
    "#### **Hidden Layers and Network Depth**\n",
    "\n",
    "The hidden layers in a neural network perform the computations necessary for feature extraction and representation learning. The number of hidden layers and the number of neurons in each layer, referred to as the network's depth, are hyperparameters that can be adjusted based on the complexity of the problem and the available computational resources.\n",
    "\n",
    "#### **Deep Neural Networks**\n",
    "\n",
    "Deep neural networks refer to neural networks with multiple hidden layers. Deep learning has gained significant attention due to the ability of deep neural networks to learn hierarchical representations and solve complex problems in areas such as computer vision, natural language processing, and speech recognition.\n",
    "\n",
    "Neural network architecture plays a crucial role in the performance and capabilities of the model. Choosing the right architecture, including the number of layers, the number of neurons, and the activation functions, is essential for achieving optimal results in various machine learning and deep learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d519e4",
   "metadata": {},
   "source": [
    "### **Neural Network Prediction for Regression and Classification**\n",
    "\n",
    "Neural networks are versatile models that can be used for both regression and classification tasks. The prediction process differs slightly depending on the type of problem being addressed.\n",
    "\n",
    "#### **Regression Prediction**\n",
    "\n",
    "In regression tasks, the goal is to predict a continuous numerical value as the output. Here's how neural networks make predictions for regression:\n",
    "\n",
    "1. **Feedforward Propagation**: The input data is passed through the neural network in the forward direction. Each neuron in the network receives inputs from the previous layer, computes the weighted sum of inputs, applies an activation function, and passes the output to the next layer. This process continues until the output layer is reached.\n",
    "\n",
    "2. **Output Layer**: In regression, the output layer typically consists of a single neuron that produces a continuous numerical value. The activation function used in the output layer depends on the specific requirements of the problem. For example, a ReLU function is commonly used for regression tasks.\n",
    "\n",
    "3. **Final Prediction**: The output value of the neural network's output neuron represents the predicted value for the regression task. It can be interpreted as the model's estimation or approximation of the target value based on the given input.\n",
    "\n",
    "#### **Classification Prediction**\n",
    "\n",
    "In classification tasks, the goal is to assign input data to specific categories or classes. Neural networks can perform multi-class or binary classification. Here's how neural networks make predictions for classification:\n",
    "\n",
    "1. **Feedforward Propagation**: Similar to regression, the input data is propagated through the network in the forward direction. Each neuron computes the weighted sum of inputs, applies an activation function, and passes the output to the next layer.\n",
    "\n",
    "2. **Output Layer**: In classification, the output layer depends on the number of classes in the problem. For binary classification, the output layer typically consists of a single neuron using a sigmoid activation function, which produces a value between 0 and 1 representing the probability of belonging to the positive class. For multi-class classification, the output layer may consist of multiple neurons using softmax activation, where each neuron represents the probability of belonging to a specific class.\n",
    "\n",
    "3. **Final Prediction**: In binary classification, the predicted class can be determined based on a threshold value (e.g., 0.5). If the output probability is above the threshold, the sample is classified as the positive class; otherwise, it is classified as the negative class. In multi-class classification, the class with the highest predicted probability is assigned as the predicted class.\n",
    "\n",
    "Neural networks learn the mapping between the input data and the desired output through the training process, where the weights and biases are adjusted to minimize the error between the predicted output and the actual target values. Once trained, the neural network can be used to make predictions on new, unseen data.\n",
    "\n",
    "Understanding how neural networks make predictions in regression and classification tasks is crucial for interpreting the model's outputs and evaluating its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef62721",
   "metadata": {},
   "source": [
    "### **Recurrent Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b01094",
   "metadata": {},
   "source": [
    "RNN were created because there were a few issues in the feed-forward neural network:\n",
    "\n",
    "Cannot handle sequential data\n",
    "Considers only the current input\n",
    "Cannot memorize previous inputs\n",
    "The solution to these issues is the RNN. An RNN can handle sequential data, accepting the current input data, and previously received inputs. RNNs can memorize previous inputs due to their internal memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d15ec7e",
   "metadata": {},
   "source": [
    "RNN works on the principle of saving the output of a particular layer and feeding this back to the input in order to predict the output of the layer.\n",
    "\n",
    "Below is how you can convert a Feed-Forward Neural Network into a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939692b",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn.png\" \n",
    "     align=\"center\" \n",
    "     width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7dbff",
   "metadata": {},
   "source": [
    "The nodes in different layers of the neural network are compressed to form a single layer of recurrent neural networks. A, B, and C are the parameters of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec2671c",
   "metadata": {},
   "source": [
    "<img src=\"./images/rnn_animation.gif\" \n",
    "     align=\"center\" \n",
    "     width=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582a4990",
   "metadata": {},
   "source": [
    "The four commonly used types of Recurrent Neural Networks are:\n",
    "\n",
    "**One-to-One**: The simplest type of RNN is One-to-One, which allows a single input and a single output. It has fixed input and output sizes and acts as a traditional neural network. The One-to-One application can be found in Image Classification.\n",
    "\n",
    "**One-to-Many**: One-to-Many is a type of RNN that gives multiple outputs when given a single input. It takes a fixed input size and gives a sequence of data outputs. Its applications can be found in Music Generation and Image Captioning.\n",
    "\n",
    "**Many-to-One**: Many-to-One is used when a single output is required from multiple input units or a sequence of them. It takes a sequence of inputs to display a fixed output. Sentiment Analysis is a common example of this type of Recurrent Neural Network.\n",
    "\n",
    "**Many-to-Many**: Many-to-Many is used to generate a sequence of output data from a sequence of input units.\n",
    "\n",
    "This type of RNN is further divided into ﻿the following two subcategories:\n",
    "\n",
    "    1. Equal Unit Size: In this case, the number of both the input and output units is the same. A common application can be found in Name-Entity Recognition.\n",
    "    2. Unequal Unit Size: In this case, inputs and outputs have different numbers of units. Its application can be found in Machine Translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a8d520",
   "metadata": {},
   "source": [
    "<img src=\"./images/types_rnn.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94067480",
   "metadata": {},
   "source": [
    "### **LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202225c",
   "metadata": {},
   "source": [
    "Now, even though RNNs are quite powerful, they suffer from Vanishing gradient problem which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: Long Short Term Networks(LSTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df53f1",
   "metadata": {},
   "source": [
    "**What is Vanishing Gradient problem?**\n",
    "\n",
    "Vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n",
    "\n",
    "<img src=\"./images/decay.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5b40d",
   "metadata": {},
   "source": [
    "**Fixing the Vanishing/Exploding Gradient with LSTMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dc68dd",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n",
    "\n",
    "The expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af99c9",
   "metadata": {},
   "source": [
    "<img src=\"./images/lstm.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114acb76",
   "metadata": {},
   "source": [
    "More on LSTMs: https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe06da-9d21-4545-80d3-7f0f0b28f9e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Sentiment Detection with (Many to One) LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c4aef7-c740-4e22-b3c7-f0c7ad800e6b",
   "metadata": {},
   "source": [
    "Previously we looked at one-to-one RNN type, which is the basic structure. And next one is one-to-many type. For example, if the model gets the fixed format like image as an input, it generates the sequence data. You can see the implementation on image caption application. Another type is many-to-many type. It gets sequence data as an inputs, and also generates the sequence data as an output. Common application of many-to-many type is machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a11b510-2b47-4541-ac3f-18d7d9682afe",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/many-to-one.png\" \n",
    "     align=\"center\" \n",
    "     width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38511973-d6b4-4c85-8804-0fb303e2b24d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Many-to-one type, which is our topic in this post, gets an sequence data as an input and generates some informatic data like labels. So we can use it for classification. Suppose that someone defines the sentiment of each sentence, and train the model with many-to-one type. And when the model gets the unseen sentence, then it will predict the intention of sentence, good or bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820903c3-4b7b-493c-91bd-710551de1030",
   "metadata": {},
   "source": [
    "Suppose we have a sentence, **\"This movie is interesting\"**. And we want to classify the sentiment of this sentence. In order to do this, we need to apply tokenization in word level. If this sentensce intends the good sentiment, then word token may contains good words, like \"good\". So we can classify this sentence to good sentiment.\n",
    "\n",
    "So if we want to apply it in RNN model, we need to consider the sentence as a word sequence (many), then classify its label (one). That is process of many-to-one type model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a4e0b3-0340-4880-b799-cbc402c1b47c",
   "metadata": {},
   "source": [
    "<img src=\"./images/many-to-one_detail.png\" \n",
    "     align=\"center\" \n",
    "     width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38dea48-6cf7-4994-8c5c-d19bd0b659e9",
   "metadata": {},
   "source": [
    "## **Reading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97bf1eef-09cd-4d5a-b74e-b5bfedac54b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment  \\\n",
       "0                  1   \n",
       "1                  0   \n",
       "2                  0   \n",
       "3                  0   \n",
       "4                  1   \n",
       "\n",
       "                                                                                                                                       text  \n",
       "0                                                                  @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "1            @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "2                                                                                   @VirginAmerica and it's a really big bad thing about it  \n",
       "3  @VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's really the only bad thing about flying VA  \n",
       "4                                                           @VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/airline_sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4142758-24ea-4e83-9127-33826e81181c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>airline_sentiment</th>\n",
       "      <td>11541.0</td>\n",
       "      <td>0.204748</td>\n",
       "      <td>0.403535</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count      mean       std  min  25%  50%  75%  max\n",
       "airline_sentiment  11541.0  0.204748  0.403535  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc631e4-2cfc-46f3-adde-0628554ca5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9178\n",
       "1    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3358906-8689-4424-9c35-fe488b75ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11541 entries, 0 to 11540\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   airline_sentiment  11541 non-null  int64 \n",
      " 1   text               11541 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 180.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e1285-70ca-4c4c-b907-8a0dc2051cb5",
   "metadata": {},
   "source": [
    "## **Pre Processing data for training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf02e38-3e13-429a-9151-fc8efd3b4120",
   "metadata": {},
   "source": [
    "- Normalise sentences: remove special characters, convert to lower case etc.\n",
    "- Remove stop words to give more relevance to specific words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d052c",
   "metadata": {},
   "source": [
    "**Normalizing Sentences:**\n",
    "\n",
    "Normalizing sentences is an important step in text preprocessing for sentiment analysis and text generation tasks. It involves applying various techniques to standardize the text data and make it more uniform. Here are some reasons why normalizing sentences is done:\n",
    "\n",
    "    * Remove Special Characters: Special characters such as punctuation marks, symbols, or emojis do not contribute much to the sentiment or meaning of a sentence. By removing these special characters, we can focus on the essential words and improve the efficiency of the subsequent analysis steps.\n",
    "\n",
    "    * Convert to Lower Case: Converting all text to lower case helps in reducing the dimensionality of the data. It ensures that the same words appearing in different cases (e.g., \"good\" and \"Good\") are treated as the same word during analysis. This step avoids duplication and helps in capturing the overall sentiment more accurately.\n",
    "\n",
    "    * Uniformity and Consistency: Normalizing sentences ensures that the text data is consistent and follows a standard format. It helps in creating a uniform representation of words and phrases, allowing the subsequent models to learn patterns and relationships effectively.\n",
    "\n",
    "**Removing Stop Words:**\n",
    "\n",
    "Stop words are commonly occurring words in a language that do not carry much meaning or contribute significantly to the sentiment of a sentence. Examples of stop words include \"a,\" \"the,\" \"and,\" \"is,\" and so on. Here are the reasons why removing stop words is done:\n",
    "\n",
    "    * Relevance to Specific Words: Stop words occur frequently in the text but often do not add much value to sentiment analysis or text generation. Removing stop words can help in focusing on the more meaningful and informative words, providing better context for sentiment analysis and generating more relevant and coherent text.\n",
    "\n",
    "    * Noise Reduction: Stop words can introduce noise and unwanted variation in the text data. By eliminating them, we can reduce the noise level and improve the signal-to-noise ratio. This leads to better analysis results and more accurate predictions.\n",
    "\n",
    "    * Reducing Dimensionality: Stop words are typically high-frequency words that appear in almost every sentence. By removing them, we can reduce the dimensionality of the data and improve computational efficiency during model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b126e338-4896-43cd-a4c3-2a3d8abce629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading English stop words\n",
    "stop = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3988683c-09bf-456f-a308-4ef5070dbb23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_process_text_data(text: str) -> str:\n",
    "    # normalize and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^0-9a-zA-Z]+',' ',text)\n",
    "    # remove stop words\n",
    "    words = text.split()\n",
    "    words = [w for w in words if (w not in stop)]\n",
    "    words = ' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98fb0090-d947-4e5d-9bcb-935291c1ed56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica plus added commercials experience tacky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica seriously would pay 30 flight seats playing really bad thing flying va</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica yes nearly every time fly vx ear worm go away</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment  \\\n",
       "0                  1   \n",
       "1                  0   \n",
       "2                  0   \n",
       "3                  0   \n",
       "4                  1   \n",
       "\n",
       "                                                                                             text  \n",
       "0                                           virginamerica plus added commercials experience tacky  \n",
       "1  virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse  \n",
       "2                                                              virginamerica really big bad thing  \n",
       "3            virginamerica seriously would pay 30 flight seats playing really bad thing flying va  \n",
       "4                                     virginamerica yes nearly every time fly vx ear worm go away  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(pre_process_text_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d978232-d848-43f3-a48a-6cac90c18f9a",
   "metadata": {},
   "source": [
    "## **Checking most used words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b14a6004-9fcf-45ea-9623-aff5bd10a3b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "for i, review in enumerate(df['text']):\n",
    "    counts.update(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbd30f06-a179-43cd-b8a5-0aeab410b7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united',\n",
       " 'flight',\n",
       " 'usairways',\n",
       " 'americanair',\n",
       " 'southwestair',\n",
       " 'jetblue',\n",
       " 'get',\n",
       " 'cancelled',\n",
       " 'thanks',\n",
       " 'service']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb92d579-609b-4636-b199-4e1a728e7046",
   "metadata": {},
   "source": [
    "## **Creating numeric representation of words (Bag of Words)**\n",
    "\n",
    "The bag-of-words representation is a technique used to convert text data into a numerical representation that can be used for machine learning algorithms. It disregards the order and structure of words in a sentence and focuses solely on the frequency of occurrence of words. Here's an overview of the steps involved in creating a bag-of-words representation:\n",
    "\n",
    "    * Tokenization: The text is split into individual words or tokens. Each token represents a unit of meaning, such as a word or a combination of words.\n",
    "\n",
    "    * Vocabulary Creation: All unique tokens in the text data are collected to create a vocabulary or a dictionary. This vocabulary serves as the set of all possible features for the bag-of-words representation.\n",
    "\n",
    "    * Frequency Count: For each document or sentence in the text data, the frequency of each token in the vocabulary is counted. This creates a numerical representation of the text data, where each entry represents the frequency of a specific token in a particular document.\n",
    "\n",
    "    * Checking Sequence Length and Padding: In some cases, the length of different sentences or documents may vary. To handle this, the sequence length is checked, and padding is applied to ensure that all sequences have the same length. \n",
    "\n",
    "The resulting bag-of-words representation is typically a matrix where each row represents a document or sentence, and each column corresponds to a specific token in the vocabulary. The values in the matrix represent the frequency or occurrence of each token in the corresponding document.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Document  | Word1 | Word2 | Word3 | Word4 | Word5 |\n",
    "|-----------|-------|-------|-------|-------|-------|\n",
    "| Sentence1 | 2     | 1     | 0     | 1     | 0     |\n",
    "| Sentence2 | 0     | 1     | 1     | 0     | 1     |\n",
    "| Sentence3 | 1     | 0     | 0     | 1     | 1     |\n",
    "| Sentence4 | 0     | 1     | 0     | 0     | 0     |\n",
    "\n",
    "In this example, we have four sentences (Sentence1, Sentence2, Sentence3, and Sentence4) with five unique words (Word1, Word2, Word3, Word4, and Word5). The table represents the bag-of-words representation, where each entry indicates the frequency of the corresponding word in the respective sentence. For instance, in Sentence1, Word1 occurs twice, Word2 occurs once, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3ca371c-e4de-4f64-89f5-61108c021809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_to_int = {word: i for i, word in enumerate(words, start=1)}\n",
    "int_to_word = {i: word for i, word in enumerate(words, start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd6dea37-2c71-44ce-8402-8c1cc5238ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_int(text:str, word_to_int: dict):\n",
    "    return [word_to_int[word] for word in text.split()] #word_to_int[key] = assigned digit\n",
    "\n",
    "\n",
    "def int_to_text(int_arr, int_to_word: dict):\n",
    "    return ' '.join([int_to_word[index] for index in int_arr if index != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a158248-7fe8-43c2-9ef6-84f7e10b3e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapped_reviews = []\n",
    "for review in df['text']:\n",
    "   # print(review, text_to_int(review, word_to_int))\n",
    "    mapped_reviews.append(text_to_int(review, word_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b3aa841-1724-4750-9c53-87eb2a34a5e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: virginamerica plus added commercials experience tacky\n",
      "Mapped text: [44, 450, 1142, 2233, 100, 5429]\n"
     ]
    }
   ],
   "source": [
    "print(f'Original text: {df.loc[0][\"text\"]}')\n",
    "print(f'Mapped text: {mapped_reviews[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eec94556-0a3f-40d9-ac69-6bae85c21bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "length_sent = []\n",
    "for i in range(len(mapped_reviews)):\n",
    "    length_sent.append(len(mapped_reviews[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4fa1722-2196-4fd7-b185-5de987e93399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>length_sent</th>\n",
       "      <th>mapped_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>virginamerica plus added commercials experience tacky</td>\n",
       "      <td>6</td>\n",
       "      <td>[44, 450, 1142, 2233, 100, 5429]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse</td>\n",
       "      <td>11</td>\n",
       "      <td>[44, 57, 3036, 3845, 3846, 815, 3037, 3847, 19, 366, 2234]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "      <td>5</td>\n",
       "      <td>[44, 57, 328, 114, 367]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>virginamerica seriously would pay 30 flight seats playing really bad thing flying va</td>\n",
       "      <td>13</td>\n",
       "      <td>[44, 281, 33, 184, 132, 2, 119, 1991, 57, 114, 367, 77, 1818]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>virginamerica yes nearly every time fly vx ear worm go away</td>\n",
       "      <td>11</td>\n",
       "      <td>[44, 107, 1201, 199, 14, 50, 2235, 3848, 5430, 76, 339]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>americanair flight cancelled flightled leaving tomorrow morning auto rebooked tuesday night flight need arrive monday</td>\n",
       "      <td>15</td>\n",
       "      <td>[4, 2, 8, 30, 376, 98, 175, 936, 170, 705, 162, 2, 37, 478, 560]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>americanair right cue delays</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 125, 3829, 161]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>americanair thank got different flight chicago</td>\n",
       "      <td>6</td>\n",
       "      <td>[4, 22, 34, 287, 2, 362]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>americanair leaving 20 minutes late flight warnings communication 15 minutes late flight called shitty customer svc</td>\n",
       "      <td>16</td>\n",
       "      <td>[4, 376, 198, 58, 35, 2, 4817, 462, 257, 58, 35, 2, 163, 1528, 12, 804]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>americanair money change flight answer phones suggestions make commitment</td>\n",
       "      <td>9</td>\n",
       "      <td>[4, 251, 82, 2, 208, 663, 1790, 71, 2143]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        text  \\\n",
       "0                                                                      virginamerica plus added commercials experience tacky   \n",
       "1                             virginamerica really aggressive blast obnoxious entertainment guests faces amp little recourse   \n",
       "2                                                                                         virginamerica really big bad thing   \n",
       "3                                       virginamerica seriously would pay 30 flight seats playing really bad thing flying va   \n",
       "4                                                                virginamerica yes nearly every time fly vx ear worm go away   \n",
       "...                                                                                                                      ...   \n",
       "11536  americanair flight cancelled flightled leaving tomorrow morning auto rebooked tuesday night flight need arrive monday   \n",
       "11537                                                                                           americanair right cue delays   \n",
       "11538                                                                         americanair thank got different flight chicago   \n",
       "11539    americanair leaving 20 minutes late flight warnings communication 15 minutes late flight called shitty customer svc   \n",
       "11540                                              americanair money change flight answer phones suggestions make commitment   \n",
       "\n",
       "       length_sent  \\\n",
       "0                6   \n",
       "1               11   \n",
       "2                5   \n",
       "3               13   \n",
       "4               11   \n",
       "...            ...   \n",
       "11536           15   \n",
       "11537            4   \n",
       "11538            6   \n",
       "11539           16   \n",
       "11540            9   \n",
       "\n",
       "                                                                mapped_reviews  \n",
       "0                                             [44, 450, 1142, 2233, 100, 5429]  \n",
       "1                   [44, 57, 3036, 3845, 3846, 815, 3037, 3847, 19, 366, 2234]  \n",
       "2                                                      [44, 57, 328, 114, 367]  \n",
       "3                [44, 281, 33, 184, 132, 2, 119, 1991, 57, 114, 367, 77, 1818]  \n",
       "4                      [44, 107, 1201, 199, 14, 50, 2235, 3848, 5430, 76, 339]  \n",
       "...                                                                        ...  \n",
       "11536         [4, 2, 8, 30, 376, 98, 175, 936, 170, 705, 162, 2, 37, 478, 560]  \n",
       "11537                                                      [4, 125, 3829, 161]  \n",
       "11538                                                 [4, 22, 34, 287, 2, 362]  \n",
       "11539  [4, 376, 198, 58, 35, 2, 4817, 462, 257, 58, 35, 2, 163, 1528, 12, 804]  \n",
       "11540                                [4, 251, 82, 2, 208, 663, 1790, 71, 2143]  \n",
       "\n",
       "[11541 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'text': df['text'],\n",
    "        'length_sent': length_sent,\n",
    "        'mapped_reviews': mapped_reviews}\n",
    "\n",
    "df_new = pd.DataFrame(data)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d75bf5-0024-4998-80b5-471af44be311",
   "metadata": {},
   "source": [
    "## **Checking sequence length and padding accordingly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bbf03aea-686d-43a5-87a7-4e87acb094f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequence_length = max(length_sent)\n",
    "X = pad_sequences(maxlen = sequence_length, \n",
    "                  sequences = mapped_reviews, \n",
    "                  padding = \"post\", \n",
    "                  value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a69d6817-07ac-429e-a3ba-1462f8d45246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12532"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "918a9dc5-55c7-4edc-b8d0-37fb34aafa53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  44,  450, 1142, 2233,  100, 5429,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a62fc56-7eb0-4f9d-ac69-76f6bf3e466d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df['airline_sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccdd1fc-aa41-44d4-966e-92e25d20df3f",
   "metadata": {},
   "source": [
    "## **Creating LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "94a94d87-19a4-49a3-9478-4254fada66e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "max_review_length = max(length_sent) #26\n",
    "no_of_words = len(words) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b0235d3-4ec3-4d2e-ba15-ae255d13ea45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12530"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b21f4b95-583e-4985-af99-5365223129b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=no_of_words, output_dim=32, input_length = 26)) #26 is max no of words in a review\n",
    "model.add(LSTM(40, return_sequences=True))\n",
    "model.add(LSTM(40, return_sequences=False))\n",
    "model.add(Dense(2, activation='softmax')) \n",
    "\n",
    "#Softmax activation is commonly used for multi-class classification problems. \n",
    "#It converts raw scores into probabilities for each class\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ee269b2-3df0-4371-aa7a-a70c225ec7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 26, 32)            401056    \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 26, 40)            11680     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 40)                12960     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 82        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 425,778\n",
      "Trainable params: 425,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65953e9-6e92-41ad-820b-3b5020245f9f",
   "metadata": {},
   "source": [
    "## **Preparing data for training and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a9848c86-31b7-4f36-ae64-cc4468f90071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c36922c-6f1e-41bd-ae76-30e4136c56ff",
   "metadata": {},
   "source": [
    "## **One-hot-encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "50ddca1a-8d90-4f5b-957f-b26cb85f0539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7570e633-fc26-4e66-b1b1-2248fa6f3fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f6659-5c51-4a63-9b58-27d6591271bf",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18a3b18c-aa25-48ff-b59a-4c8a5795216c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "253/253 [==============================] - 10s 41ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.5777 - val_accuracy: 0.8975\n",
      "Epoch 2/15\n",
      "253/253 [==============================] - 10s 40ms/step - loss: 0.0073 - accuracy: 0.9986 - val_loss: 0.6362 - val_accuracy: 0.8920\n",
      "Epoch 3/15\n",
      "253/253 [==============================] - 11s 43ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.6196 - val_accuracy: 0.8802\n",
      "Epoch 4/15\n",
      "253/253 [==============================] - 11s 45ms/step - loss: 0.0127 - accuracy: 0.9970 - val_loss: 0.4841 - val_accuracy: 0.9070\n",
      "Epoch 5/15\n",
      "253/253 [==============================] - 11s 43ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.5994 - val_accuracy: 0.8972\n",
      "Epoch 6/15\n",
      "253/253 [==============================] - 11s 45ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.5861 - val_accuracy: 0.8975\n",
      "Epoch 7/15\n",
      "253/253 [==============================] - 11s 44ms/step - loss: 0.0102 - accuracy: 0.9983 - val_loss: 0.6029 - val_accuracy: 0.8828\n",
      "Epoch 8/15\n",
      "253/253 [==============================] - 11s 43ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 0.4347 - val_accuracy: 0.8978\n",
      "Epoch 9/15\n",
      "253/253 [==============================] - 12s 48ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.6290 - val_accuracy: 0.8984\n",
      "Epoch 10/15\n",
      "253/253 [==============================] - 12s 46ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.5464 - val_accuracy: 0.8966\n",
      "Epoch 11/15\n",
      "253/253 [==============================] - 12s 46ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.6095 - val_accuracy: 0.8946\n",
      "Epoch 12/15\n",
      "253/253 [==============================] - 12s 46ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.6315 - val_accuracy: 0.8995\n",
      "Epoch 13/15\n",
      "253/253 [==============================] - 11s 44ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.7127 - val_accuracy: 0.8946\n",
      "Epoch 14/15\n",
      "253/253 [==============================] - 11s 43ms/step - loss: 0.0098 - accuracy: 0.9984 - val_loss: 0.6596 - val_accuracy: 0.8923\n",
      "Epoch 15/15\n",
      "253/253 [==============================] - 12s 46ms/step - loss: 0.0103 - accuracy: 0.9979 - val_loss: 0.6124 - val_accuracy: 0.8934\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9dc89ea9-5ece-44ea-80bd-2ce4157a0da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "acc_values = history_dict['accuracy']\n",
    "val_acc_values = history_dict['val_accuracy']\n",
    "epochs = range(1, len(val_loss_values) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "157d56ca-3c44-42a9-ada9-c02a3d484c38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(211)\n",
    "plt.plot(epochs, history.history['loss'], 'bo', label='Training loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c8ae7e0e-8999-49d4-9f62-084612262504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(212)\n",
    "plt.plot(epochs, history.history['accuracy'], 'bo', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.gca().set_yticklabels(['{:.0f}%'.format(x * 100) for x in plt.gca().get_yticks()]) \n",
    "plt.legend()\n",
    "plt.grid('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec1114-2b17-48dc-a357-5c46673cc87c",
   "metadata": {},
   "source": [
    "## **Anecdotes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "38cceeeb-b31a-41ed-ae72-b221d7d7d834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sentiment(model, text) -> str:\n",
    "    text_int_embedding = text_to_int(text, word_to_int)\n",
    "    text_int_embedding = pad_sequences(maxlen = sequence_length, \n",
    "                                       sequences = [text_int_embedding], \n",
    "                                       padding = \"post\", value = 0)\n",
    "    sentiment_index = np.argmax(model.predict(text_int_embedding))\n",
    "    return sentiment_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "729bb8a0-a6dd-4371-9727-997f9c50458e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 3s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "result = np.argmax(model.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "710bcbfc-7528-4d7a-bc35-ca71913ee41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_sentences = [int_to_text(embedding, int_to_word) for i, embedding in enumerate(X_test) if result[i] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "fdef917a-c279-4d57-b11f-058560bf7f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negative_sentences = [int_to_text(embedding, int_to_word) for i, embedding in enumerate(X_test) if result[i] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cec25941-c298-469a-a099-79dfdb9b7e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['southwestair leeannhealey yeah sale fares got places fly oh damn right live swa fly',\n",
       " 'jetblue news gate options',\n",
       " 'jetblue utah think thanks',\n",
       " 'americanair let extraordinary week make year remember goingforgreat 2015 thanks much american airlines',\n",
       " 'usairways talked reservation must congratulation friendly good usair airline fly']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c6597156-a767-4065-8e23-c26d12d71590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jetblue flight flight booking problems experience pretty great',\n",
       " 'usairways glad airline going swallowed american american always picks phone solves problems',\n",
       " 'southwestair disconnected call 2 5 hours without even speaking octaviannightmare',\n",
       " 'americanair well done taken fun air travel phlairport',\n",
       " 'usairways told coded upgrade clearly purchased seat miles refuse downgrade ripoff']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70e236-27b3-4947-aaa1-7645ddb3a16b",
   "metadata": {},
   "source": [
    "## **Generating Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b77cd-ea53-4e2c-911e-f463b2b52f45",
   "metadata": {},
   "source": [
    "The strategy that we'll adopt to generate text is as follows:\n",
    "\n",
    "1. Import project Gutenberg's Alice's Adventures in Wonderland dataset, which can be downloaded from https://www.gutenberg.org/files/11/11-0.txt.\n",
    "2. Preprocess the text data so that we bring every word to the same case, and remove punctuation.\n",
    "3. Assign an ID to each unique word and then convert the dataset into a sequence of word IDs.\n",
    "4. Loop through the total dataset, 10 words at a time. Consider the 10 words as input and the subsequent 11th word as output.\n",
    "5. Build and train a model, by performing embedding on top of the input word IDs and then connecting the embeddings to an LSTM, which is connected to the output layer through a hidden layer. The value in the output layer is the one-hot-encoded version of the output.\n",
    "6. Make a prediction for the subsequent word by taking a random location of word and consider the historical words prior to the location of the random word chosen.\n",
    "7. Move the window of the input words by one from the seed word's location that we chose earlier and the tenth time step word shall be the word that we predicted in the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439fb39",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "Input Preparation:\n",
    "\n",
    "    The input text is preprocessed by converting it to lowercase and removing any non-alphanumeric characters.\n",
    "    The text is then split into individual words and stored in a list.\n",
    "\n",
    "Vocabulary Building:\n",
    "\n",
    "    The vocabulary is constructed by counting the occurrences of each unique word in the text.\n",
    "    Words are sorted based on their frequency, with more frequent words having higher priority.\n",
    "    Each word is assigned a unique index, creating a mapping between words and their respective indices.\n",
    "\n",
    "Sequence Generation:\n",
    "\n",
    "    To train the model, we need input-output pairs in the form of sequences.\n",
    "    A sliding window approach is employed, where a fixed-length sequence of words (input sequence) is paired with the subsequent word (output word).\n",
    "    By sliding the window over the text with a specified step size, multiple input-output pairs are generated.\n",
    "\n",
    "Encoding the Data:\n",
    "\n",
    "    To feed the data into the LSTM model, we need to encode the input and output sequences into a suitable format.\n",
    "    One-hot encoding is applied to represent each word as a binary vector, indicating whether or not a specific word is present in the sequence.\n",
    "    The input data is converted into a 3D array, where each element represents the presence or absence of a word at a specific position in the sequence.\n",
    "    The output data is converted into a 2D array, where each row corresponds to the one-hot encoded representation of the output word.\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "    The LSTM model is constructed with a many-to-one architecture.\n",
    "    The input layer receives the encoded input sequences.\n",
    "    The LSTM layer processes the input sequences, capturing the temporal dependencies and learning the patterns in the data.\n",
    "    The LSTM layer outputs a fixed-size representation (hidden state) of the input sequence.\n",
    "    The hidden state is passed through a dense (fully connected) layer with a softmax activation function.\n",
    "    The softmax layer generates a probability distribution over the vocabulary, indicating the likelihood of each word being the next word in the sequence.\n",
    "\n",
    "Training the Model:\n",
    "\n",
    "    The model is trained using the input-output pairs generated from the text data.\n",
    "    During training, the model learns to minimize the difference between the predicted output and the actual output (one-hot encoded representation of the next word).\n",
    "    The model's parameters are updated iteratively using an optimizer (in this case, Adam) and a loss function (categorical cross-entropy).\n",
    "\n",
    "Generating Text:\n",
    "\n",
    "    To generate text, we provide a seed sequence as input to the trained model.\n",
    "    The model predicts the next word in the sequence based on the input.\n",
    "    The predicted word is appended to the input sequence, and the process is repeated to generate subsequent words.\n",
    "    By iteratively predicting the next word, we can generate text that follows the patterns and style learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "97dfabac-8153-47e1-8e63-5f5fb864d5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(filename: str = '../data/alice.txt'):\n",
    "    with open(filename, encoding='utf-8-sig') as fin:\n",
    "        lines = []\n",
    "        for line in fin:\n",
    "            line = line.strip().lower()\n",
    "            if (len(line) == 0):\n",
    "                continue\n",
    "            lines.append(line)\n",
    "        fin.close()\n",
    "        text = \" \".join(lines)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "924ea3e5-666e-4cc5-bd3a-f54d52ab5a34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f78a57f8-779c-4b53-b0d9-78961433798e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moment to think about stopping herself before she found herself falling down a very deep well. either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next. first, she tried to look down and make out what she was coming to, but it was too dark to see anything; then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves; here and there she saw maps '"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[3001:3500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9468a7-6e27-40db-9a5f-257354c5567e",
   "metadata": {},
   "source": [
    "Normalize the text to remove punctuations and convert it to lowercase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3aaefc7c-1473-4f1b-b680-1598c9333368",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pre_process(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f81900-f446-4f46-8a72-97184ac0eaab",
   "metadata": {},
   "source": [
    "Assign the unique words to an index so that they can be referenced when constructing the training and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f4925b6b-e392-4da8-9462-2814fa094d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = Counter()\n",
    "counts.update(text.split())\n",
    "\n",
    "words = sorted(counts, key=counts.get, reverse=True)\n",
    "nb_words = len(text.split())\n",
    "\n",
    "word2index = {word: i for i, word in enumerate(words)}\n",
    "index2word = {i: word for i, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb195e3-9e8a-43a8-844a-cd08fb4c081c",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b26eadbf-2f36-490f-91d0-124c697623c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEQLEN = 10\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dfe608-000a-46a4-b5cb-e84608757b4f",
   "metadata": {},
   "source": [
    "**Construct the input set of words that leads to an output word. Note that we are considering a sequence of 10 words and trying to predict the 11th word**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2f1a0752-1e80-4c00-bd24-311606840604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_and_labels(text: str, \n",
    "                         seq_length: int = SEQLEN, \n",
    "                         step: int = STEP):\n",
    "\n",
    "    input_words = []\n",
    "    label_words = []\n",
    "\n",
    "    text_arr = text.split()\n",
    "\n",
    "    for i in range(0, nb_words-seq_length, step):\n",
    "        x = text_arr[i:(i+seq_length)]\n",
    "        y = text_arr[i+seq_length]\n",
    "        input_words.append(x)\n",
    "        label_words.append(y)\n",
    "\n",
    "    return input_words, label_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8c6371cc-71c4-4052-84b0-5c7febd4be19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_words, label_words = get_input_and_labels(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dd4eca8f-3228-4d58-accd-aed4dfc810e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: the project gutenberg ebook of alice’s adventures in wonderland, by\n",
      "Output: lewis\n"
     ]
    }
   ],
   "source": [
    "print(f'Input: {\" \".join(input_words[0])}\\nOutput: {label_words[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6337ca2-b447-4159-8482-b7bd536db594",
   "metadata": {},
   "source": [
    "Construct the vectors of the input and the output datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8c2c532-534e-47ce-988d-8eabb7beaa1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_words = len(set(words))\n",
    "\n",
    "X = np.zeros((len(input_words), SEQLEN, total_words), dtype= bool)\n",
    "y = np.zeros((len(input_words), total_words), dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28507c81-3a88-4c1b-ac83-b52a01a1d91b",
   "metadata": {},
   "source": [
    "We are creating empty arrays in the preceding step, which will be populated in the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "70ca2513-711b-42da-b337-f3a0b3633dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create encoded vectors for the input and output values\n",
    "for i, input_word in enumerate(input_words):\n",
    "    for j, word in enumerate(input_word):\n",
    "        X[i, j, word2index[word]] = 1\n",
    "        y[i, word2index[label_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc5186a-1f98-4970-9bff-7a859332dc70",
   "metadata": {},
   "source": [
    "In the preceding code, the first for loop is used to loop through all the words in the input sequence of words (10 words in input), and the second for loop is used to loop through an individual word in the chosen sequence of input words. Additionally, given that the output is a list, we do not need to update it using the second for loop (as there is no sequence of IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2bb4c415-5f03-41b3-b769-a326c8151984",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of X: (29584, 10, 5649)\n",
      "Input of y: (29584, 5649)\n"
     ]
    }
   ],
   "source": [
    "print(f'Input of X: {X.shape}\\nInput of y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd11a2-e619-4d2a-bb49-b737ada7956c",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e0280a03-5b7d-4a93-ba4d-d0ec2c6aee2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_ITERATIONS = 100\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "016337a9-2f6f-4a91-bf70-c2104887e777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 128)               2958336   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5649)              728721    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,687,057\n",
      "Trainable params: 3,687,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, total_words)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d175afbf-5fd7-4683-9908-f3f1c84f87ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Fit the model**. \n",
    "- Look at how the output varies over an increasing number of epochs. \n",
    "- Generate a random set of sequences of 10 words and try to predict the next possible word. \n",
    "- We are in a position to observe how our predictions are getting better over an increasing number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff9b2d20-7abb-4ff7-8b5a-182544a11cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_model_output(model, preds: int, input_words, seq_length, total_words):\n",
    "    test_idx = np.random.randint(int(len(input_words)*0.1)) * (-1)\n",
    "    test_words = input_words[test_idx]\n",
    "\n",
    "    for curr_pred in range(preds):\n",
    "        curr_embedding = np.zeros((1, seq_length, total_words))\n",
    "\n",
    "        for i, ch in enumerate(test_words):\n",
    "            curr_embedding[0, i, word2index[ch]] = 1\n",
    "\n",
    "        pred = model.predict(curr_embedding, verbose=0)[0]\n",
    "        word_pred = index2word[np.argmax(pred)]\n",
    "\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Prediction {curr_pred + 1} of {preds}\")\n",
    "        print(f'Generating from seed: {\" \".join(test_words)}\\nNext Word: {word_pred}')\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        test_words = test_words[1:] + [word_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c14f7545-afb1-412a-ae50-25c5b92b3daa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833/833 [==============================] - 278s 327ms/step - loss: 7.0355 - val_loss: 8.4122\n",
      "==================================================\n",
      "Prediction 1 of 5\n",
      "Generating from seed: do not agree to be bound by the terms of\n",
      "Next Word: the\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 2 of 5\n",
      "Generating from seed: not agree to be bound by the terms of the\n",
      "Next Word: the\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 3 of 5\n",
      "Generating from seed: agree to be bound by the terms of the the\n",
      "Next Word: the\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 4 of 5\n",
      "Generating from seed: to be bound by the terms of the the the\n",
      "Next Word: the\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 5 of 5\n",
      "Generating from seed: be bound by the terms of the the the the\n",
      "Next Word: the\n",
      "==================================================\n",
      "833/833 [==============================] - 332s 399ms/step - loss: 6.5705 - val_loss: 8.8803\n",
      "833/833 [==============================] - 450s 541ms/step - loss: 6.2685 - val_loss: 8.9289\n",
      "833/833 [==============================] - 431s 518ms/step - loss: 5.7963 - val_loss: 9.2351\n",
      "833/833 [==============================] - 315s 378ms/step - loss: 5.2847 - val_loss: 9.5950\n",
      "833/833 [==============================] - 357s 429ms/step - loss: 4.7751 - val_loss: 10.0509\n",
      "833/833 [==============================] - 382s 458ms/step - loss: 4.2536 - val_loss: 10.8979\n",
      "833/833 [==============================] - 392s 470ms/step - loss: 3.7356 - val_loss: 11.0099\n",
      "833/833 [==============================] - 400s 480ms/step - loss: 3.2173 - val_loss: 11.5891\n",
      "833/833 [==============================] - 394s 473ms/step - loss: 2.7246 - val_loss: 12.2080\n",
      "833/833 [==============================] - 397s 477ms/step - loss: 2.2535 - val_loss: 12.4818\n",
      "==================================================\n",
      "Prediction 1 of 5\n",
      "Generating from seed: to, the full project gutenberg-tm license must appear prominently whenever\n",
      "Next Word: and\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 2 of 5\n",
      "Generating from seed: the full project gutenberg-tm license must appear prominently whenever and\n",
      "Next Word: are\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 3 of 5\n",
      "Generating from seed: full project gutenberg-tm license must appear prominently whenever and are\n",
      "Next Word: are\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 4 of 5\n",
      "Generating from seed: project gutenberg-tm license must appear prominently whenever and are are\n",
      "Next Word: him\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 5 of 5\n",
      "Generating from seed: gutenberg-tm license must appear prominently whenever and are are him\n",
      "Next Word: you\n",
      "==================================================\n",
      "833/833 [==============================] - 404s 485ms/step - loss: 1.8125 - val_loss: 13.0930\n",
      "833/833 [==============================] - 402s 482ms/step - loss: 1.4134 - val_loss: 13.3517\n",
      "833/833 [==============================] - 411s 493ms/step - loss: 1.0795 - val_loss: 13.8078\n",
      "833/833 [==============================] - 411s 493ms/step - loss: 0.7985 - val_loss: 14.4013\n",
      "833/833 [==============================] - 411s 494ms/step - loss: 0.5834 - val_loss: 14.7829\n",
      "833/833 [==============================] - 411s 494ms/step - loss: 0.4124 - val_loss: 15.0893\n",
      "833/833 [==============================] - 410s 492ms/step - loss: 0.2933 - val_loss: 15.3737\n",
      "833/833 [==============================] - 403s 484ms/step - loss: 0.2087 - val_loss: 15.4577\n",
      "833/833 [==============================] - 404s 486ms/step - loss: 0.1497 - val_loss: 15.8214\n",
      "833/833 [==============================] - 401s 481ms/step - loss: 0.1039 - val_loss: 16.1409\n",
      "==================================================\n",
      "Prediction 1 of 5\n",
      "Generating from seed: it takes a considerable effort, much paperwork and many fees\n",
      "Next Word: to\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 2 of 5\n",
      "Generating from seed: takes a considerable effort, much paperwork and many fees to\n",
      "Next Word: her:\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 3 of 5\n",
      "Generating from seed: a considerable effort, much paperwork and many fees to her:\n",
      "Next Word: this\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 4 of 5\n",
      "Generating from seed: considerable effort, much paperwork and many fees to her: this\n",
      "Next Word: when\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 5 of 5\n",
      "Generating from seed: effort, much paperwork and many fees to her: this when\n",
      "Next Word: the\n",
      "==================================================\n",
      "833/833 [==============================] - 406s 488ms/step - loss: 0.0874 - val_loss: 16.1467\n",
      "833/833 [==============================] - 409s 491ms/step - loss: 0.0761 - val_loss: 16.4993\n",
      "833/833 [==============================] - 408s 490ms/step - loss: 0.0627 - val_loss: 17.1386\n",
      "833/833 [==============================] - 414s 497ms/step - loss: 0.0427 - val_loss: 17.2454\n",
      "833/833 [==============================] - 422s 507ms/step - loss: 0.0564 - val_loss: 16.8072\n",
      "833/833 [==============================] - 418s 502ms/step - loss: 0.0629 - val_loss: 17.2676\n",
      "833/833 [==============================] - 415s 498ms/step - loss: 0.0331 - val_loss: 17.2323\n",
      "833/833 [==============================] - 420s 504ms/step - loss: 0.0196 - val_loss: 17.6614\n",
      "833/833 [==============================] - 423s 508ms/step - loss: 0.0118 - val_loss: 17.8307\n",
      "833/833 [==============================] - 427s 513ms/step - loss: 0.0101 - val_loss: 17.9450\n",
      "==================================================\n",
      "Prediction 1 of 5\n",
      "Generating from seed: owns a compilation copyright in the collection of project gutenberg-tm\n",
      "Next Word: i’m\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 2 of 5\n",
      "Generating from seed: a compilation copyright in the collection of project gutenberg-tm i’m\n",
      "Next Word: seen\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 3 of 5\n",
      "Generating from seed: compilation copyright in the collection of project gutenberg-tm i’m seen\n",
      "Next Word: of\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 4 of 5\n",
      "Generating from seed: copyright in the collection of project gutenberg-tm i’m seen of\n",
      "Next Word: the\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 5 of 5\n",
      "Generating from seed: in the collection of project gutenberg-tm i’m seen of the\n",
      "Next Word: own.\n",
      "==================================================\n",
      "833/833 [==============================] - 435s 522ms/step - loss: 0.0895 - val_loss: 17.0047\n",
      "833/833 [==============================] - 447s 536ms/step - loss: 0.0479 - val_loss: 17.2063\n",
      "833/833 [==============================] - 447s 537ms/step - loss: 0.0118 - val_loss: 17.6851\n",
      "833/833 [==============================] - 451s 541ms/step - loss: 0.0056 - val_loss: 17.7935\n",
      "833/833 [==============================] - 452s 542ms/step - loss: 0.0044 - val_loss: 17.8759\n",
      "833/833 [==============================] - 452s 542ms/step - loss: 0.0039 - val_loss: 18.2224\n",
      "833/833 [==============================] - 454s 545ms/step - loss: 0.0041 - val_loss: 17.9847\n",
      "833/833 [==============================] - 452s 543ms/step - loss: 0.1235 - val_loss: 17.1215\n",
      "833/833 [==============================] - 450s 541ms/step - loss: 0.0257 - val_loss: 17.2670\n",
      "833/833 [==============================] - 455s 547ms/step - loss: 0.0065 - val_loss: 17.3965\n",
      "==================================================\n",
      "Prediction 1 of 5\n",
      "Generating from seed: the project gutenberg-tm name associated with the work. you can\n",
      "Next Word: themselves\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 2 of 5\n",
      "Generating from seed: project gutenberg-tm name associated with the work. you can themselves\n",
      "Next Word: “that’s\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 3 of 5\n",
      "Generating from seed: gutenberg-tm name associated with the work. you can themselves “that’s\n",
      "Next Word: a\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 4 of 5\n",
      "Generating from seed: name associated with the work. you can themselves “that’s a\n",
      "Next Word: little\n",
      "==================================================\n",
      "==================================================\n",
      "Prediction 5 of 5\n",
      "Generating from seed: associated with the work. you can themselves “that’s a little\n",
      "Next Word: stop\n",
      "==================================================\n",
      "833/833 [==============================] - 457s 548ms/step - loss: 0.0039 - val_loss: 17.5918\n",
      "833/833 [==============================] - 451s 542ms/step - loss: 0.0034 - val_loss: 17.9092\n",
      "833/833 [==============================] - 453s 544ms/step - loss: 0.0031 - val_loss: 17.8097\n",
      "833/833 [==============================] - 451s 541ms/step - loss: 0.0285 - val_loss: 16.7355\n",
      "833/833 [==============================] - 449s 539ms/step - loss: 0.0782 - val_loss: 16.9746\n",
      "833/833 [==============================] - 465s 559ms/step - loss: 0.0110 - val_loss: 17.1284\n",
      "833/833 [==============================] - 487s 585ms/step - loss: 0.0056 - val_loss: 17.5233\n",
      "833/833 [==============================] - 475s 571ms/step - loss: 0.0040 - val_loss: 17.5138\n",
      "833/833 [==============================] - 454s 545ms/step - loss: 0.0030 - val_loss: 17.7160\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(50):\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION, validation_split = 0.1)\n",
    "    if iteration % 10 == 0: #checks if current iteration is divisible by 10 without any remainder\n",
    "        check_model_output(model, 5, input_words, SEQLEN, total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ef015f03-b7a0-4f6c-9f19-67ec752e3ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_next_word(model, input_text: str, seq_length, total_words, temperature = None):\n",
    "    curr_embedding = np.zeros((1, seq_length, total_words))\n",
    "    \n",
    "    for i, ch in enumerate(input_text):\n",
    "        curr_embedding[0, i, word2index[ch]] = 1\n",
    "    \n",
    "    pred = model.predict(curr_embedding, verbose=0)[0]\n",
    "    \n",
    "    if temperature == None:\n",
    "        word_pred = index2word[np.argmax(pred)]\n",
    "    else:\n",
    "        next_word_token = tf.random.categorical(tf.expand_dims(pred / temperature, 0), num_samples=1)[-1, 0].numpy()\n",
    "        word_pred = index2word[next_word_token]\n",
    "    \n",
    "    return pred, word_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609074f-d056-4020-b47a-c41d5986cd71",
   "metadata": {},
   "source": [
    "## **The Problem with Text Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8d90d-ea12-4cda-ac8e-33539f6c119f",
   "metadata": {},
   "source": [
    "Language is not always the same and/or stationary, the next word to predict depends on the context, style and etc. Therefore, natural language, uses a wide variety of words, sometimes, with same meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e769a-174d-4fad-86f9-51823a26104c",
   "metadata": {},
   "source": [
    "Lets have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0760f980-a1b9-4f78-898c-8dee56a291ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the people that walk with their heads downward! the\n"
     ]
    }
   ],
   "source": [
    "test_words = input_words[-28701]\n",
    "print(' '.join(test_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0eb6f039-3935-4d8f-a323-ee44a12fdefd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits, word_pred = predict_next_word(model, test_words, SEQLEN, total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "02743517-f9e0-42dd-a01b-b0dbc6fcf993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: antipathies,\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicted word: {word_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "212b2e39-0f0b-44ee-9820-0d6aa42374bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_paragraph(model, seed, words: int, temperature: int):\n",
    "    full_text = seed.copy()\n",
    "    for _ in range(words):\n",
    "        logits, word_pred = predict_next_word(model, seed, SEQLEN, total_words, temperature=temperature)\n",
    "        seed = (seed + [word_pred])[-10:]\n",
    "        full_text = full_text + [word_pred]\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5558a777-a0d6-4390-9211-835c93f48497",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the people that walk with their heads downward! the antipathies, i think—” (she was rather glad there _was_ no one listening,\n",
      "among the people that walk with their heads downward! the antipathies, i think—” (she was rather glad there _was_ no one listening,\n",
      "among the people that walk with their heads downward! the antipathies, i think—” (she was rather glad there _was_ no one listening,\n",
      "among the people that walk with their heads downward! the antipathies, i think—” (she was rather glad there _was_ no one listening,\n",
      "among the people that walk with their heads downward! the antipathies, i think—” (she was rather glad there _was_ no one listening,\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(' '.join(generate_paragraph(model, test_words, 12, None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f72c5-1302-4488-911c-aec3c23602c0",
   "metadata": {},
   "source": [
    "## **Randomness through Entropy Scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10933f-1b52-42d4-bd95-15e9fb3ed99e",
   "metadata": {},
   "source": [
    "### What is Entropy in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf013c-1564-4fc8-97c0-248e8e736769",
   "metadata": {},
   "source": [
    "In Machine Learning, entropy is a measure of the level of disorder or uncertainty in a given dataset or system. It is a metric that quantifies the amount of information in a dataset, and it is commonly used to evaluate the quality of a model and its ability to make accurate predictions. Entropy is based on the concept of probability and is calculated using the formula -sum(p*log2(p)), where p is the probability of each possible outcome. In decision trees, entropy is used to determine the best split at each node and improve the overall accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d16513-6b05-4d4c-a5a6-522d1d032ec1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/entropy.png\" \n",
    "     align=\"center\" \n",
    "     width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f0ca90-38fd-48a4-a1aa-8c781992c2ea",
   "metadata": {},
   "source": [
    "We can understand the term entropy with any simple example: flipping a coin. When we flip a coin, then there can be two outcomes. However, it is difficult to conclude what would be the exact outcome while flipping a coin because there is no direct relation between flipping a coin and its outcomes. There is a 50% probability of both outcomes; then, in such scenarios, entropy would be high. This is the essence of entropy in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "50b3406d-a4f6-4d93-b95a-5c1dfaf0c16d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.99969137, 5.4448614e-05, 4.150259e-05, 3.558188e-05, 2.5768437e-05]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(logits, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c079d1-b570-4cfc-b941-753afd9ea5cc",
   "metadata": {},
   "source": [
    "## Softmax Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74382afb-4ef7-4809-930b-c9fb598381fd",
   "metadata": {},
   "source": [
    "Temperature is a hyperparameter of LSTMs (and neural networks generally) used to control the randomness of predictions by scaling the logits before applying softmax. Temperature scaling has been widely used to improve performance for NLP tasks that utilize the Softmax decision layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3149852-bd26-4f9e-8675-f4c1591cd072",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/example_softmax_1.png\" \n",
    "     align=\"center\" \n",
    "     width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8212b1-7a52-420f-980b-b44a13893925",
   "metadata": {},
   "source": [
    "The generated sequence will have a predictable and generic structure. And the reason is less entropy or randomness in the softmax distribution, in the sense that the likelihood of a particular word (corresponding to index 9 in the above example) getting chosen is way higher than the other words. A sequence being predictable is not problematic as long as the aim is to get realistic sequences. But if the goal is to generate a novel text or an image which has never been seen before, randomness is the holy grail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdba9ae-d1c1-4be4-bae1-db66399065e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./images/temp_vs_no_temp.png\" \n",
    "     align=\"center\" \n",
    "     width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c47738d-5e49-4493-aced-326d5ae19eea",
   "metadata": {},
   "source": [
    "<img src=\"./images/temp_animation.gif\" \n",
    "     align=\"center\" \n",
    "     width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de38d35f-2d9c-437c-895d-2b57905dd04c",
   "metadata": {},
   "source": [
    "The distribution above approaches uniform distribution giving each word an equal probability of getting sampled out, thereby rendering a more creative look to the generated sequence. Too much creativity isn’t good either. In the extreme case, the generated text might not make sense at all. Hence, like all other hyperparameters, this needs to be tuned as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ac396-9d3a-46ca-a312-488d5773ef7d",
   "metadata": {},
   "source": [
    "## Predicting using the Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e1db4279-8aad-48ec-a4ac-a4177a2b8a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among the people that walk with their heads downward! the queer, remarks,” “—change contemptuous distributed: flying child: pretty interrupted, clever?” courage. nibbling\n",
      "among the people that walk with their heads downward! the letters. herself note-book, attends fountains. encouraged pretexts ***** (before provisions. “hadn’t though),\n",
      "among the people that walk with their heads downward! the tied lessons, indirect, dinah, riddles.—i snail, too, maximum alive _you_ confused originator\n",
      "among the people that walk with their heads downward! the “fifteenth,” knock, remarked: way:— arthur paris sentenced dunce? whatever,” wandered “get caterpillar’s\n",
      "among the people that walk with their heads downward! the manage that’s growls hall, quietly, “talking unfolded is—‘oh, highest asking! soo—oop! accident,\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(' '.join(generate_paragraph(model, test_words, 12, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b82846",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this project, we embarked on an exciting journey into sentiment analysis and text generation using many-to-one LSTMs. We began by diving into sentiment detection, training an LSTM model to analyze airline sentiments based on text reviews. Through data preprocessing, bag-of-words representation, and the many-to-one LSTM architecture, we successfully predicted sentiment labels with high accuracy.\n",
    "\n",
    "Moving on to text generation, we leveraged the iconic literary work \"Alice's Adventures in Wonderland\" to train many-to-one LSTMs to generate contextually relevant text. We tackled the challenges of text generation, including language variability and word choice, and employed techniques such as entropy scaling and softmax temperature to control randomness and enhance the diversity of the generated text.\n",
    "\n",
    "Throughout the project, we explored various concepts and techniques, from preprocessing textual data to training and evaluating LSTM models. We gained insights into sentiment analysis, understanding the significance of analyzing sentiments in textual data. Additionally, we delved into the intricacies of text generation, honing our skills in next-word prediction and generating coherent sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306012ae-f900-4e28-963c-12cd527b9ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
